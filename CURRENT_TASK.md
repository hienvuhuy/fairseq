# Current Task
* This file only keeps 3 days of history

## July 12
* Running command 
  * Normal:
    * CUDA_VISIBLE_DEVICES=0 fairseq-train --task mytranslation  data-bin/iwslt14.tokenized.de-en --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric
  * Customize task:
    * CUDA_VISIBLE_DEVICES=0 fairseq-train --task mytranslation  data-bin/iwslt14.tokenized.de-en --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 4096 --eval-bleu --eval-bleu-args '{"beam": 5, "max_len_a": 1.2, "max_len_b": 10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --eval-bleu-print-samples --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --eval-bleu-print-samples-with-last-sentence
    * CUDA_VISIBLE_DEVICES=1 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m00:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m01:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m02:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m03:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m04:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m05:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m06:/home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m07 --task mytranslation --arch transformer_wmt_en_de --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1000 --keep-last-epochs 5 --eval-bleu --eval-bleu-args '{"beam":4,"max_len_a":1.2,"max_len_b":10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --save-dir /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/checkpoints/voita-en-ru-1_5m --num-workers 4 --update-freq 8 --wandb-project voita-en-ru-1_5m --seed 13234 --eval-bleu-print-samples
  
    * Note that, in debug mode, we are using transformer_iwslt_de_en, not transformer_wmt_en_de
      * CUDA_VISIBLE_DEVICES=2 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m00 --task mytranslation --arch transformer_iwslt_de_en --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1000 --keep-last-epochs 5 --eval-bleu --eval-bleu-args '{"beam":4,"max_len_a":1.2,"max_len_b":10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --save-dir /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/checkpoints/voita-en-ru-1_5m --num-workers 4 --eval-bleu-print-samples-with-last-sentence
      
    * My model
      * CUDA_VISIBLE_DEVICES=2 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m00 --task mytranslation --arch standard_transformer --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1000 --keep-last-epochs 5 --eval-bleu --eval-bleu-args '{"beam":4,"max_len_a":1.2,"max_len_b":10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --best-checkpoint-metric bleu --maximize-best-checkpoint-metric --save-dir /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/checkpoints/voita-en-ru-1_5m --num-workers 4 --eval-bleu-print-samples-with-last-sentence
      * CUDA_VISIBLE_DEVICES=2 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/baseline1_5_original/data-bin/voita-en-ru-1_5m00 --task mytranslation --arch standard_transformer --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1000 --eval-bleu --eval-bleu-args '{"beam":4,"max_len_a":1.2,"max_len_b":10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --num-workers 4 --eval-bleu-print-samples-with-last-sentence --wandb-project test-drive --seed 13234

* Visit line 281 of faireq_cli/train.py to check wandb
* Visit fairseq/logging/progress_bar.py to customize wandb with new log
* Just need to call WandBProgressBarWrapper->log (here progress.log)
* Check metric.py. The problem is now turning to:
  * Find the bleu score
  * Should we log it back immediately by the metric function?
  * Check trainer.py line 904

  * Debug
    * CUDA_VISIBLE_DEVICES=2 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/debug/data-bin/voita-en-ru-1_5m00 --task mytranslation --arch standard_transformer --optimizer adam --adam-betas '(0.9,0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.1 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 1000 --eval-bleu --eval-bleu-args '{"beam":4,"max_len_a":1.2,"max_len_b":10}' --eval-bleu-detok moses --eval-bleu-remove-bpe --num-workers 4 --eval-bleu-print-samples-with-last-sentence --wandb-project test-drive --seed 13234