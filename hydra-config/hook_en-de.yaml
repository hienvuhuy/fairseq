# @package _group_
# CUDA_VISIBLE_DEVICES=1 fairseq-train /home/cl/huyhien-v/Workspace/MT/experiments/baseline6m_split_200k/data-bin/baseline6m_split_200k29 

hydra:
    run:
        dir: .
common:
    fp16: false
    bf16: false

dataset:
    num_workers: 8
    max_tokens: 4096
    fixed_validation_seed: 1102

optimization:
    clip_norm: 0.0
    lr: 
        - 7e-4

checkpoint:
    save_dir: /cl/work2/huyhien-v/Experiments/MT/debug/en-de
    best_checkpoint_metric: bleu
    maximize_best_checkpoint_metric: true
    keep_last_epochs: 30


model:
    _name: transformer_origin
    activation_fn: "relu"
    share_decoder_input_output_embed: true
    dropout: 0.3
    # attention_dropout: 0.2
    # activation_dropout: 0.2
    
task:
    _name: doc-translation
    data: /home/is/huyhien-v/Data/Bin/MT/en-de/iwslt14/data-bin/iwslt14.tokenized.de-en
    eval_bleu: true
    eval_bleu_remove_bpe: '@@ '
    eval_bleu_args: '{"beam": 4, "lenpen": 0.6}'
    eval_bleu_detok: 'space'

criterion:
    _name: label_smoothed_cross_entropy
    label_smoothing: 0.1

optimizer:
    _name: adam
    adam_betas: "(0.9,0.98)"
    weight_decay: 0.0001

lr_scheduler:
    _name: inverse_sqrt
    warmup_updates: 4000
